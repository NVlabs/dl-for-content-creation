<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Deep Learning for Content Creation</title>
  <meta name="author" content="Orazio Gallo">
  <meta name="keywords" content="deep learning, tutorial, content creation, GAN, generative adversarial networks">
  <link rel="stylesheet" href="og_style.css">
  <link rel="stylesheet" media="screen" href="https://fontlibrary.org/face/linux-libertine" type="text/css"/>
</head>

<body>


<div align="center">

<div class="title">
    <h1>Deep Learning for Content Creation Tutorial</h1>
</div>

<div>
    <h4>In conjunction with CVPR 2019</h4>
    <img src="imgs/superslowmo.jpg" height="159" alt="superslowmo" class="center">
    <img src="imgs/dogs.jpg" height="159" alt="munit" class="center">
    <img src="https://blogs.nvidia.com/wp-content/uploads/2019/03/Waterfill-capture_blog-crop-1280x680.jpg" height="159" alt="GauGAN" class="center">
</div>

<div class="textsection">
  <h2>Goal of the tutorial</h2>
  
  <p>
    Content creation has several important applications ranging from virtual reality, videography, gaming, and even retail and advertising.
    The recent progress of deep learning and machine learning techniques allows to turn hours of manual, painstaking content creation work into minutes or seconds of automated work.
    This tutorial has several goals.
    First, it will cover some introductory concepts to help interested researchers from other fields get started in this exciting new area. Second, it will present selected success cases to advertise how deep learning can be used for content creation.
    More broadly, it will serve as a forum to discuss the latest topics in content creation and the challenges that vision and learning researchers can help solve.
  </p>
  
  <h3>Organizers:</h3>
    <a href="https://research.nvidia.com/person/deqing-sun">Deqing Sun</a>, 
    <a href="https://research.nvidia.com/person/mingyu-liu">Ming-Yu Liu</a>, 
    <a href="https://research.nvidia.com/person/orazio-gallo">Orazio Gallo</a>, and 
    <a href="https://research.nvidia.com/person/jan-kautz">Jan Kautz</a>

  <h3>Sponsors:</h3>
  <div align="center">
    <img src="imgs/nvidia_logo.png" height="100" alt="NVIDIA" class="center">
    <img src="imgs/google_logo.png" height="100" alt="Google" class="center">
  </div>  
  
  <h3>Date, time, and location:</h3>
    See you on Sunday June 16th at 8:45 in room 204.

  <h3>Recordings</h3>
    You can now see all the talks <a href="https://www.youtube.com/watch?v=jA77x_Ef28M&list=PLp85pM5EwmwZwYJLk8LHwOMZL57zmzCfX" target="_blank">here</a>, or click on the video of one specific talk below!</br>
    Unfortunately, there were some technical glitches and a couple of videos are only partial, we apologize for the inconvenience.
</div>

<div class="sectitle">
    <h2>News</h2>
</div>
<div class="textsection">
    <ul>
      <li>We have a great line-up of confirmed speakers, see program below!</li>
      <li>The tutorial was very successful, we would like to thank the speakers and the audience again!</li>
    </ul>
</div>

<div class="textsection">
  <div align="center">
    <span>
      <h2>Tentative program</h2>
      <p>
        <table style="width:95%">
            <td></td>
            <td bgcolor="#DDDDDD">Talk Title</td>
            <td bgcolor="#DDDDDD">Speaker</td> 
          </tr>

          <tr>
            <td bgcolor="#DDDDDD">8:45 - 9:00</td>
            <td>Opening words [<a href="https://youtu.be/jA77x_Ef28M" target="_blank">Video</a>]</td>
            <td><a href="https://scholar.google.com/citations?user=t4rgICIAAAAJ&hl=en" target="_blank">Deqing Sun</td>
          </tr>

<!--           <tr>
            <td height="50"></td>
            <th colspan="2">Part I</th>
          </tr>
-->

          <tr>
            <td bgcolor="#DDDDDD">9:00 - 9:45</td>
            <td>Paired and Unpaired Image Translation with GANs  [<a href="https://youtu.be/mwyeuGhfocY" target="_blank">Video</a>]
              [<a href="https://phillipi.github.io/talks/im2im_tutorial_cvpr2019.pdf" target="_blank">Slides</a>]
               </td>
            <td><a href="https://web.mit.edu/phillipi/" target="_blank">Phillip Isola</a></td> 
          </tr>
<!-- 
          <tr>
            <td height="50"></td>
            <th colspan="2">Part II</th>
          </tr>
 -->

          <tr>
            <td bgcolor="#DDDDDD">9:45 - 10:10</td>                        
            <td>Improving Image Translation [<a href="https://youtu.be/0hC1Gkfn-aw" target="_blank">Video</a>][<a href="https://1drv.ms/p/s!AmX25rlPIA7hjsJ9a0wxUvwqba-baw" target="_blank">Slides</a>]</td>
            <td><a href="http://jamestompkin.com/" target="_blank">James Tompkin</a></td> 
          </tr>

          <tr>
            <td>10:10 - 10:25</td>
            <th colspan="2">Coffee break (15m)</th>
          </tr>


          <tr>
            <td bgcolor="#DDDDDD">10:25 - 10:50</td>
            <td>Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation [<a href="https://youtu.be/mVsUYMFMmUI" target="_blank">Video</a>]</td>
            <td><a href="https://people.cs.umass.edu/~hzjiang/" target="_blank">Huaizu Jiang</a></td>
          </tr>

          <tr>
            <td bgcolor="#DDDDDD">10:50 - 11:15</td>
            <td>FUNIT: Few-Shot Unsupervised Image-to-Image Translation [<a href="https://youtu.be/IhAsXcCz8LI" target="_blank">Video</a>]</td>
            <td><a href="https://research.nvidia.com/person/mingyu-liu" target="_blank">Ming-Yu Liu</a></td>
          </tr>

          <tr>
            <td bgcolor="#DDDDDD">11:15 - 11:40</td>
            <td>A Style-Based Generator Architecture for Generative Adversarial Networks [<a href="https://youtu.be/qJPptQk-ArI" target="_blank">Video</a>]</td>
            <td><a href="https://research.nvidia.com/person/tero-karras" target="_blank">Tero Karras</a></td>
          </tr>

          <tr>
            <td bgcolor="#DDDDDD">11:40 - 12:05</td>
            <td>Meta-Sim: Learning to Generate Synthetic Datasets [<a href="https://youtu.be/JK9vtH3KA4g" target="_blank">Video</a>]</td>
            <td><a href="https://www.cs.utoronto.ca/~fidler/" target="_blank">Sanja Fidler</a></td>
          </tr>
          
          <tr>
            <td>12:05 - 13:20</td>
            <th colspan="2">Lunch break (1h 15m)</th>
          </tr>

          <tr>
            <td bgcolor="#DDDDDD">13:20 - 13:45</td>
            <td>GauGAN: Semantic Image Synthesis with Spatially Adaptive Normalization  [<a href="https://youtu.be/XKFp0bFG1tE" target="_blank">Video</a>]</td>
            <td><a href="https://taesung.me/" target="_blank">Taesung Park</a></td>
          </tr>

<!--           <tr>
            <td height="50"></td>
            <th colspan="2">Part III</th>
          </tr>
-->

          <tr>
            <td bgcolor="#DDDDDD">13:45 - 14:10</td>
            <td>Style Transfer: Application to Photos and Paintings [<a href="https://youtu.be/uSp5-ezLNus" target="_blank">Video</a>]</td>
            <td><a href="http://people.csail.mit.edu/sparis/" target="_blank">Sylvain Paris</a></td>
          </tr>

          <tr>
            <td bgcolor="#DDDDDD">14:10 - 14:35</td>
            <td>vid2vid: Video-to-Video Synthesis [No Video]</td>
            <td><a href="https://tcwang0509.github.io/" target="_blank">Ting-Chun Wang</a></td>
          </tr>

          <tr>
            <td>14:35 - 14:50</td>
            <th colspan="2">Coffee break (15m)</th>
          </tr>

          <tr>
            <td bgcolor="#DDDDDD">14:50 - 15:15</td>
            <td>Visualizing and Understanding GANs [<a href="" target="_blank">Video</a>]</td>
            <td><a href="http://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a></td>
          </tr>

          <tr>
            <td bgcolor="#DDDDDD">15:15 - 15:40</td>
            <td>Content Creation by Inserting Objects [<a href="https://youtu.be/n3aoyzRtOF4" target="_blank">Video</a>] [<a href="https://drive.google.com/file/d/1JR7Xm35yvpKTE_BwBddq874j9hPsbdx8/view?ts=5d091d6a" target="_blank">Slides</a>]</td>
            <td><a href="https://research.nvidia.com/person/sifei-liu" target="_blank">Sifei Liu</a> and <a href="http://rllab.snu.ac.kr/people/donghoon-lee" target="_blank">Donghoon Lee</a></td>
          </tr>

          <tr>
            <td bgcolor="#DDDDDD">15:40 - 16:05</td>
            <td>Recent Advances in Neural and Example-based Stylization [<a href="https://youtu.be/kBf8IoqTYe4" target="_blank">Video</a>]</td>
            <td><a href="https://research.adobe.com/person/eli-shechtman/" target="_blank">Eli Shechtman</a></td>
          </tr>

          <tr>
            <td>16:05 - 16:20</td>
            <th colspan="2">Coffee break (15m)</th>
          </tr>

          <tr>
            <td bgcolor="#DDDDDD">16:20 - 17:30</td>
            <td colspan="2">Panel discussion with Phillip Isola, </br> Jun-Yan Zhu, </br> Huaizu Jiang,  and James Tompkin</td>
          </tr>

        </table>
      </p>
    </span>
  </div>
</div>


<div class="vgap">
</div>

</div>


</body>
</html>
